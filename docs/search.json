[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3"
  },
  {
    "objectID": "lab6.html#i.-visualizing-relationships",
    "href": "lab6.html#i.-visualizing-relationships",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "I. Visualizing Relationships",
    "text": "I. Visualizing Relationships\n\nQuestion 1: Your Turn -\n\nData and PDF is downloaded and in my /data directory.\n\n\nzero_q_frequency is when the frequency of days with Q = 0 mm/day.\n\n\n\nExploratory Data Analysis -\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Your Turn -\n\nMap of Aridity\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"salmon\") +\n  labs(title = \"Map of Gauge Aridity Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nMap of p_mean\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat, color = p_mean)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point() +\n  scale_color_gradient(low = \"palegreen\", high = \"#FF69B4\") +\n  labs(title = \"Map of Gauge p_mean Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nCombined Map\n\ncamels_long &lt;- camels %&gt;%\n  pivot_longer(cols = c(aridity, p_mean), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Create the faceted plot\nggplot(data = camels_long, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = value)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"pink\") +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(title = \"Map of Gauge Values in the US\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#ii.-model-preparation",
    "href": "lab6.html#ii.-model-preparation",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "II. Model Preparation",
    "text": "II. Model Preparation\n\nRelationship between aridity, rainfall, and mean flow.\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# Visual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Test Log Transformation Graph\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Visualizing Log Transform with q_mean added\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nA. Model Building\n\nSplitting the Data\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\nPreprocessors: recipe\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nNaive base lm approach\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCorrect Version: prep -&gt; bake -&gt; predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: Statistical and visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing a workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n\nMaking Predictions\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61"
  },
  {
    "objectID": "lab6.html#iii.-model-evaluation",
    "href": "lab6.html#iii.-model-evaluation",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "III. Model Evaluation",
    "text": "III. Model Evaluation\n\nStatistical & Visual\n\nDefine/Extract Default metrics\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\n\nScatter plot to visualize\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\nSwitch it up\n\nRandom Forest model to predict streamflow\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nA workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n\n\n\nQuestion 3: Your Turn!\n\nBuild a xgboost (engine) regression (mode) model using boost_tree & a neural network model using bag_mlp()\n\n# XGBoost model\nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nxg_workflow &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(xg_model) |&gt;\n  fit(data = camels_train)\n\n# Neural net model\nnn_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\nnn_workflow &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(nn_model) |&gt;\n  fit(data = camels_train)\n\n\n\nEvaluate these models\n\n# Making Predictions\nxg_preds &lt;- predict(xg_workflow, new_data = camels_test) |&gt;\n  bind_cols(camels_test)\n\nnn_preds &lt;- predict(nn_workflow, new_data = camels_test) |&gt;\n  bind_cols(camels_test)\n\n# XGBoost metrics\nmetrics_xg &lt;- xg_preds |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\n# Neural Net metrics\nmetrics_nn &lt;- nn_preds |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\n#Visualizing \nbind_rows(\n  xg_preds |&gt; mutate(model = \"XGBoost\"),\n  nn_preds |&gt; mutate(model = \"Neural Net\")\n) |&gt;\n  ggplot(aes(x = logQmean, y = .pred, color = model)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(x = \"Actual\", y = \"Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModel Preference: Neural Net Model\n####The neural net model has the largest r-squared value (0.759). Of the 4 models, I would utilize the Neural Network Model because it outperforms the others when considering error and explanatory power while being stable.\n\n\n\nIII. Build Your Own\n\nData Splitting\n\nset.seed(456)\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\ncamels |&gt; \n  select(soil_depth_statsgo, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n                   soil_depth_statsgo     p_mean      q_mean\nsoil_depth_statsgo         1.00000000 0.08478728 -0.02725808\np_mean                     0.08478728 1.00000000  0.88657569\nq_mean                    -0.02725808 0.88657569  1.00000000\n\n\n\n\nRecipe used: (logQmean ~ soil_depth_statgso + p_mean) to see if the depth of the soil impacts the amount of streamflow.\n\nrec &lt;-  recipe(logQmean ~ soil_depth_statsgo + p_mean, data = camels_train) %&gt;%\n  step_interact(terms = ~ soil_depth_statsgo:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ soil_depth_statsgo * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ soil_depth_statsgo * p_mean, data = baked_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9974 -0.2920  0.1105  0.4361  1.6656 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -1.94316    0.33781  -5.752 1.54e-08 ***\nsoil_depth_statsgo        -0.32909    0.26200  -1.256    0.210    \np_mean                     0.71833    0.10765   6.673 6.70e-11 ***\nsoil_depth_statsgo:p_mean -0.01738    0.08248  -0.211    0.833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6807 on 498 degrees of freedom\nMultiple R-squared:  0.6557,    Adjusted R-squared:  0.6536 \nF-statistic: 316.1 on 3 and 498 DF,  p-value: &lt; 2.2e-16\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nVisualizing\n\ncamels_long &lt;- camels %&gt;%\n  pivot_longer(cols = c(soil_depth_statsgo, p_mean), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Create the faceted plot\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = soil_depth_statsgo)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"salmon\") +\n  labs(title = \"Map of Gauge Soil Depth Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nModels -\n\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  set_mode(\"regression\")\n\n\n\nWorkflow Set\n\ncomplex_workflow &lt;- workflow_set(\n  preproc = list(simple = rec),\n  models = list(\n    linear_reg = lm_model,\n    random_forest = rf_model,\n    bagged_mlp = nn_model,\n    xgboost = xg_model\n  )\n)\n\nmodel_results &lt;- complex_workflow |&gt;\n  workflow_map(resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\n\nEvaluation\n\nautoplot(model_results)\n\n\n\n\n\n\n\nsummary(model_results)\n\n   wflow_id         info.Length  info.Class  info.Mode\n Length:4           4       tbl_df  list              \n Class :character   4       tbl_df  list              \n Mode  :character   4       tbl_df  list              \n                    4       tbl_df  list              \n    option.Length          option.Class          option.Mode     \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n  result.Length      result.Class      result.Mode   \n 4                 resample_results  list            \n 4                 resample_results  list            \n 4                 resample_results  list            \n 4                 resample_results  list            \n\nrank_results(model_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 simple_random_fo… Prepro… rmse    0.564  0.0254    10 recipe       rand…     1\n2 simple_random_fo… Prepro… rsq     0.754  0.0268    10 recipe       rand…     1\n3 simple_bagged_mlp Prepro… rmse    0.721  0.197     10 recipe       bag_…     2\n4 simple_bagged_mlp Prepro… rsq     0.739  0.0447    10 recipe       bag_…     2\n5 simple_xgboost    Prepro… rmse    0.596  0.0302    10 recipe       boos…     3\n6 simple_xgboost    Prepro… rsq     0.725  0.0301    10 recipe       boos…     3\n7 simple_linear_reg Prepro… rmse    0.679  0.0256    10 recipe       line…     4\n8 simple_linear_reg Prepro… rsq     0.659  0.0174    10 recipe       line…     4\n\n\n\n\nModel Ranking Discussion -\n\n\nWith the ranking results, the random forest is the best model to find a correlation between the variables (soil_porosity & p_mean) and stream flow.\n\n\n\nExtract and Evaluate\n\nModel Used: Random Forest\n\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 168  60\n\nrf_metrics &lt;- rf_data |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\nrf_metrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.639\n2 rsq     standard       0.731\n3 mae     standard       0.394\n\n# Plotting\n\nggplot(rf_data, aes(x = logQmean, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"lightgreen\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"darkblue\") +\n  labs(\n    title = \"Random Forest: Actual vs Predicted Streamflow (logQmean) by Soil Depth and Precipitation\",\n    x = \"Actual logQmean\",\n    y = \"Predicted logQmean\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nResult Discussion:\n\n\nAs the logQmean increases, the correlation becomes stronger for predictions of stream flow using predictors such as precipitation and soil depth. However, the model’s r-squared is below .9, meaning it is likely not successful. Although since all the models are below it from the ranking results, this means that soil_depth is not a viable predictor to stream flow."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "ESS 330 - Lab 8: Machine Learning",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\n\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\nvisdat::vis_dat(camels)\n\n\n\n\n\n\n\ncamels &lt;- camels |&gt;\n  drop_na()\n\nset.seed(123)\ncamels_split &lt;- initial_split(camels, prop = .8, strata = q_mean)\n\ncamels_training &lt;- training(camels_split)\ncamels_testing &lt;- testing(camels_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#i.-set-up-lab",
    "href": "hyperparameter-tuning.html#i.-set-up-lab",
    "title": "ESS 330 - Lab 8: Machine Learning",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\n\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\n\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\nvisdat::vis_dat(camels)\n\n\n\n\n\n\n\ncamels &lt;- camels |&gt;\n  drop_na()\n\nset.seed(123)\ncamels_split &lt;- initial_split(camels, prop = .8, strata = q_mean)\n\ncamels_training &lt;- training(camels_split)\ncamels_testing &lt;- testing(camels_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#ii.-resampling-and-model-testing",
    "href": "hyperparameter-tuning.html#ii.-resampling-and-model-testing",
    "title": "ESS 330 - Lab 8: Machine Learning",
    "section": "II. Resampling and Model Testing",
    "text": "II. Resampling and Model Testing\n\n1. Recipe and Data Baking\n\n# Recipe\nrec &lt;- recipe(logQmean ~ soil_porosity + soil_depth_pelletier + gauge_lat + gauge_lon, data = camels_training) |&gt;\n  update_role(gauge_lat, gauge_lon, new_role = \"id\") |&gt; \n  step_log(all_predictors(), offset = 1e-6) |&gt;\n  step_naomit(all_predictors(), all_outcomes())\n\n# Baking Data\nbaked_data &lt;- prep(rec, camels_training) |&gt; \n  bake(new_data = NULL)\n\n\n\n2. Building Resamples and 3 Models\n\n# Cross Validation\nfold_camels &lt;- vfold_cv(camels_training, v = 10)\n\n# Linear Model\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  set_mode(\"regression\")\n\n# Random Forest\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\n# Decision Tree\ndt_model &lt;- decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n\n\n3. Workflow Set and Testing Models\n\n# Workflow Model List\nmodel_list &lt;- list(\n  linear = lm_model,\n  random_forest = rf_model,\n  decision_tree = dt_model\n)\n\n# Workflow\ncamels_workflows &lt;- workflow_set(\n  preproc = list(basic_recipe = rec),\n  models = model_list\n)\n\n# Fit to resamples\nresults &lt;- camels_workflows |&gt;\n  workflow_map(\"fit_resamples\", resamples = fold_camels)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n# Visualize\nautoplot(results)\n\n\n\n\n\n\n\n\n\n\n4. Model Selection\n\nThe model that will best fit is the linear_reg model. The model’s range of rmse is lowest and has a decent rsq, meaning that it is making the most accurate predictions and performs the best out of the three.\n\n\nModel Selected: Linear Regression\n\nEngine: “lm”\nMode: “Regression”\nReason for choice: high accuracy of predictions and simple results based on variable connection."
  },
  {
    "objectID": "hyperparameter-tuning.html#iii.-model-tuning",
    "href": "hyperparameter-tuning.html#iii.-model-tuning",
    "title": "ESS 330 - Lab 8: Machine Learning",
    "section": "III. Model Tuning",
    "text": "III. Model Tuning\n\n1. Building a Model for Chosen Specification\n\n# Linear Model with Hyperparameters\n\ntuned_lm_model &lt;- linear_reg(\n  penalty = tune(), \n  mixture = tune()\n) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nlm_workflow &lt;- workflow() |&gt; \n  add_model(tuned_lm_model) |&gt; \n  add_recipe(rec)\n\n# Checking Tunable Values and Ranges\ndials &lt;- extract_parameter_set_dials(lm_workflow)\n\ndials$object\n\n[[1]]\n\n\nAmount of Regularization (quantitative)\n\n\nTransformer: log-10 [1e-100, Inf]\n\n\nRange (transformed scale): [-10, 0]\n\n\n\n[[2]]\n\n\nProportion of Lasso Penalty (quantitative)\n\n\nRange: [0.05, 1]\n\n\n\n\n2. Define the Search Space and Tuning Model\n\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.4.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nmy.grid &lt;- grid_space_filling(\n  dials,\n  size = 25\n)\n\nmy.grid\n\n# A tibble: 25 × 2\n    penalty mixture\n      &lt;dbl&gt;   &lt;dbl&gt;\n 1 1   e-10  0.604 \n 2 2.61e-10  0.327 \n 3 6.81e-10  0.842 \n 4 1.78e- 9  0.129 \n 5 4.64e- 9  0.485 \n 6 1.21e- 8  0.683 \n 7 3.16e- 8  0.921 \n 8 8.25e- 8  0.288 \n 9 2.15e- 7  0.0896\n10 5.62e- 7  0.525 \n# ℹ 15 more rows\n\nmodel_params &lt;-  tune_grid(\n    lm_workflow,\n    resamples = fold_camels,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\nThe model maintains consistent performance across a range of regularization strengths but experiences a notable decline when the penalty becomes very small. This indicates that regularization is crucial for maintaining stability and accuracy, although the specific form of regularization has minimal impact on performance in this scenario.\n\n\n\n3. Check Skill of Tuned Model\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n         penalty mixture .metric .estimator   mean     n std_err .config        \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1 0.000464       0.05   mae     standard   0.938     10  0.0189 Preprocessor1_…\n 2 0.000464       0.05   rmse    standard   1.22      10  0.0276 Preprocessor1_…\n 3 0.000464       0.05   rsq     standard   0.0986    10  0.0293 Preprocessor1_…\n 4 0.000000215    0.0896 mae     standard   0.938     10  0.0189 Preprocessor1_…\n 5 0.000000215    0.0896 rmse    standard   1.22      10  0.0276 Preprocessor1_…\n 6 0.000000215    0.0896 rsq     standard   0.0986    10  0.0293 Preprocessor1_…\n 7 0.00000000178  0.129  mae     standard   0.938     10  0.0189 Preprocessor1_…\n 8 0.00000000178  0.129  rmse    standard   1.22      10  0.0276 Preprocessor1_…\n 9 0.00000000178  0.129  rsq     standard   0.0987    10  0.0294 Preprocessor1_…\n10 0.0562         0.169  mae     standard   0.935     10  0.0191 Preprocessor1_…\n# ℹ 65 more rows\n\nshow_best(model_params, metric = \"mae\", n = 1)\n\n# A tibble: 1 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   0.383   0.367 mae     standard   0.925    10  0.0223 Preprocessor1_Model09\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\nThe best model is mixture, as it’s value for MAE is lower, it can make more accurate predictions and is a good fit for the data. The preprocessors’ config is Preprocessor1_Model06.\n\n\n\n4. Finalizing the Model\n\nfinalized_workflow &lt;- finalize_workflow(\n  lm_workflow,\n  hp_best\n)\n\nfinal_fit &lt;- fit(finalized_workflow, data = camels_training)\n\nfinal_res &lt;- last_fit(finalized_workflow, split = camels_split)\n\ncollect_metrics(final_res)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      1.20   Preprocessor1_Model1\n2 rsq     standard      0.0587 Preprocessor1_Model1\n\ncollect_predictions(final_res)\n\n# A tibble: 104 × 5\n     .pred id                .row logQmean .config             \n     &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n 1 -0.432  train/test split     1  0.776   Preprocessor1_Model1\n 2 -0.122  train/test split     4  0.878   Preprocessor1_Model1\n 3 -0.249  train/test split     7  0.601   Preprocessor1_Model1\n 4 -0.357  train/test split    16  0.608   Preprocessor1_Model1\n 5 -0.261  train/test split    29  0.586   Preprocessor1_Model1\n 6 -0.251  train/test split    31  0.642   Preprocessor1_Model1\n 7 -0.0956 train/test split    36  0.266   Preprocessor1_Model1\n 8 -0.0779 train/test split    44 -0.00956 Preprocessor1_Model1\n 9 -0.0942 train/test split    45 -0.00340 Preprocessor1_Model1\n10 -0.0974 train/test split    46  0.132   Preprocessor1_Model1\n# ℹ 94 more rows\n\n\n\nMetrics and Model Analyses\n\n\nAlthough the RMSE is 1.2, indicating that the predictions are reasonably close to the observed values, the very low R² suggests the model fails to capture much of the underlying variability. This implies that the selected predictors—soil porosity and depth to bedrock—are not particularly effective for modeling q_mean.\n\n\n\n5. Creating a Scatterplot\n\nfinal_res |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(x = .pred, y = logQmean)) +\n  geom_smooth(method = \"lm\",  color = \"#53868B\") +\n  geom_point(alpha = 0.6, color = \"#2F4F4F\") +\n  geom_abline(slope = 1, intercept = 0, color = \"#FF6EB4\", linetype = \"dashed\") +\n  scale_color_viridis_c(option = \"plasma\") +\n  labs(\n    x = \"Predicted\",\n    y = \"Actual\",\n    title = \"Actual vs Predicted Values for Q_mean\\nwith Predictors of Soil Porosity and Soil Depth\\nto Bedrock\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "hyperparameter-tuning.html#iv.-building-a-map-and-full-data-prediction",
    "href": "hyperparameter-tuning.html#iv.-building-a-map-and-full-data-prediction",
    "title": "ESS 330 - Lab 8: Machine Learning",
    "section": "IV. Building A Map and Full Data Prediction",
    "text": "IV. Building A Map and Full Data Prediction\n\nfull_fit &lt;- fit(finalized_workflow, data = camels)\n\naug_data &lt;- augment(full_fit, new_data = camels)\n\naug_data &lt;- aug_data |&gt;\n  mutate(residuals = (logQmean - .pred)^2)\n\n\nGGplot Maps\n\n# Map of Predictions\nmap_pred &lt;- ggplot(aug_data, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 2) +\n  coord_fixed() +\n  scale_color_viridis_c(option = \"plasma\", name = \"Predicted\\nlogQmean\") +\n  labs(title = \"Predicted logQmean\") +\n  theme_classic(base_size = 3)\n\n# Map of Residuals\nmap_resid &lt;- ggplot(aug_data, aes(x = gauge_lon, y = gauge_lat, color = residuals)) +\n  geom_point(size = 2) +\n  coord_fixed() +\n  scale_color_viridis_c(option = \"inferno\", name = \"Residuals\\n(Squared)\") +\n  labs(title = \"Residuals (Squared Error)\") +\n  theme_classic(base_size = 3)\n\n# Combine the two maps using patchwork\nfinal_plot &lt;- map_pred | map_resid +\n  plot_annotation(title = \"Predictions and Residuals Across Sites\")\n\n# Display\nfinal_plot"
  }
]