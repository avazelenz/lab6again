[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3"
  },
  {
    "objectID": "lab6.html#i.-visualizing-relationships",
    "href": "lab6.html#i.-visualizing-relationships",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "I. Visualizing Relationships",
    "text": "I. Visualizing Relationships\n\nQuestion 1: Your Turn -\n\nData and PDF is downloaded and in my /data directory.\n\n\nzero_q_frequency is when the frequency of days with Q = 0 mm/day.\n\n\n\nExploratory Data Analysis -\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Your Turn -\n\nMap of Aridity\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"salmon\") +\n  labs(title = \"Map of Gauge Aridity Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nMap of p_mean\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat, color = p_mean)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point() +\n  scale_color_gradient(low = \"palegreen\", high = \"#FF69B4\") +\n  labs(title = \"Map of Gauge p_mean Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nCombined Map\n\ncamels_long &lt;- camels %&gt;%\n  pivot_longer(cols = c(aridity, p_mean), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Create the faceted plot\nggplot(data = camels_long, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = value)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"pink\") +\n  facet_wrap(~ variable, scales = \"free\") +\n  labs(title = \"Map of Gauge Values in the US\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#ii.-model-preparation",
    "href": "lab6.html#ii.-model-preparation",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "II. Model Preparation",
    "text": "II. Model Preparation\n\nRelationship between aridity, rainfall, and mean flow.\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# Visual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Test Log Transformation Graph\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Visualizing Log Transform with q_mean added\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nA. Model Building\n\nSplitting the Data\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n\nPreprocessors: recipe\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nNaive base lm approach\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCorrect Version: prep -&gt; bake -&gt; predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: Statistical and visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing a workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n\nMaking Predictions\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61"
  },
  {
    "objectID": "lab6.html#iii.-model-evaluation",
    "href": "lab6.html#iii.-model-evaluation",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "III. Model Evaluation",
    "text": "III. Model Evaluation\n\nStatistical & Visual\n\nDefine/Extract Default metrics\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\n\nScatter plot to visualize\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\nSwitch it up\n\nRandom Forest model to predict streamflow\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n\nModel Evaluation: statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nA workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n\n\n\nQuestion 3: Your Turn!\n\nBuild a xgboost (engine) regression (mode) model using boost_tree & a neural network model using bag_mlp()\n\n# XGBoost model\nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nxg_workflow &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(xg_model) |&gt;\n  fit(data = camels_train)\n\n# Neural net model\nnn_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\nnn_workflow &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(nn_model) |&gt;\n  fit(data = camels_train)\n\n\n\nEvaluate these models\n\n# Making Predictions\nxg_preds &lt;- predict(xg_workflow, new_data = camels_test) |&gt;\n  bind_cols(camels_test)\n\nnn_preds &lt;- predict(nn_workflow, new_data = camels_test) |&gt;\n  bind_cols(camels_test)\n\n# XGBoost metrics\nmetrics_xg &lt;- xg_preds |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\n# Neural Net metrics\nmetrics_nn &lt;- nn_preds |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\n#Visualizing \nbind_rows(\n  xg_preds |&gt; mutate(model = \"XGBoost\"),\n  nn_preds |&gt; mutate(model = \"Neural Net\")\n) |&gt;\n  ggplot(aes(x = logQmean, y = .pred, color = model)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(x = \"Actual\", y = \"Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModel Preference: Neural Net Model\n####The neural net model has the largest r-squared value (0.759). Of the 4 models, I would utilize the Neural Network Model because it outperforms the others when considering error and explanatory power while being stable.\n\n\n\nIII. Build Your Own\n\nData Splitting\n\nset.seed(456)\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\ncamels |&gt; \n  select(soil_depth_statsgo, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n                   soil_depth_statsgo     p_mean      q_mean\nsoil_depth_statsgo         1.00000000 0.08478728 -0.02725808\np_mean                     0.08478728 1.00000000  0.88657569\nq_mean                    -0.02725808 0.88657569  1.00000000\n\n\n\n\nRecipe used: (logQmean ~ soil_depth_statgso + p_mean) to see if the depth of the soil impacts the amount of streamflow.\n\nrec &lt;-  recipe(logQmean ~ soil_depth_statsgo + p_mean, data = camels_train) %&gt;%\n  step_interact(terms = ~ soil_depth_statsgo:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ soil_depth_statsgo * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ soil_depth_statsgo * p_mean, data = baked_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9974 -0.2920  0.1105  0.4361  1.6656 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -1.94316    0.33781  -5.752 1.54e-08 ***\nsoil_depth_statsgo        -0.32909    0.26200  -1.256    0.210    \np_mean                     0.71833    0.10765   6.673 6.70e-11 ***\nsoil_depth_statsgo:p_mean -0.01738    0.08248  -0.211    0.833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6807 on 498 degrees of freedom\nMultiple R-squared:  0.6557,    Adjusted R-squared:  0.6536 \nF-statistic: 316.1 on 3 and 498 DF,  p-value: &lt; 2.2e-16\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nVisualizing\n\ncamels_long &lt;- camels %&gt;%\n  pivot_longer(cols = c(soil_depth_statsgo, p_mean), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Create the faceted plot\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = soil_depth_statsgo)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"salmon\") +\n  labs(title = \"Map of Gauge Soil Depth Values in the US\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nModels -\n\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() |&gt;\n  set_engine(\"nnet\") |&gt;\n  set_mode(\"regression\")\n\nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  set_mode(\"regression\")\n\n\n\nWorkflow Set\n\ncomplex_workflow &lt;- workflow_set(\n  preproc = list(simple = rec),\n  models = list(\n    linear_reg = lm_model,\n    random_forest = rf_model,\n    bagged_mlp = nn_model,\n    xgboost = xg_model\n  )\n)\n\nmodel_results &lt;- complex_workflow |&gt;\n  workflow_map(resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\n\nEvaluation\n\nautoplot(model_results)\n\n\n\n\n\n\n\nsummary(model_results)\n\n   wflow_id         info.Length  info.Class  info.Mode\n Length:4           4       tbl_df  list              \n Class :character   4       tbl_df  list              \n Mode  :character   4       tbl_df  list              \n                    4       tbl_df  list              \n    option.Length          option.Class          option.Mode     \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n 1                     workflow_set_options  list                \n  result.Length      result.Class      result.Mode   \n 4                 resample_results  list            \n 4                 resample_results  list            \n 4                 resample_results  list            \n 4                 resample_results  list            \n\nrank_results(model_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 simple_random_fo… Prepro… rmse    0.564  0.0254    10 recipe       rand…     1\n2 simple_random_fo… Prepro… rsq     0.754  0.0268    10 recipe       rand…     1\n3 simple_bagged_mlp Prepro… rmse    0.721  0.197     10 recipe       bag_…     2\n4 simple_bagged_mlp Prepro… rsq     0.739  0.0447    10 recipe       bag_…     2\n5 simple_xgboost    Prepro… rmse    0.596  0.0302    10 recipe       boos…     3\n6 simple_xgboost    Prepro… rsq     0.725  0.0301    10 recipe       boos…     3\n7 simple_linear_reg Prepro… rmse    0.679  0.0256    10 recipe       line…     4\n8 simple_linear_reg Prepro… rsq     0.659  0.0174    10 recipe       line…     4\n\n\n\n\nModel Ranking Discussion -\n\n\nWith the ranking results, the random forest is the best model to find a correlation between the variables (soil_porosity & p_mean) and stream flow.\n\n\n\nExtract and Evaluate\n\nModel Used: Random Forest\n\nrf_model &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(data = camels_train) \n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 168  60\n\nrf_metrics &lt;- rf_data |&gt;\n  metrics(truth = logQmean, estimate = .pred)\n\nrf_metrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.639\n2 rsq     standard       0.731\n3 mae     standard       0.394\n\n# Plotting\n\nggplot(rf_data, aes(x = logQmean, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"lightgreen\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"darkblue\") +\n  labs(\n    title = \"Random Forest: Actual vs Predicted Streamflow (logQmean) by Soil Depth and Precipitation\",\n    x = \"Actual logQmean\",\n    y = \"Predicted logQmean\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nResult Discussion:\n\n\nAs the logQmean increases, the correlation becomes stronger for predictions of stream flow using predictors such as precipitation and soil depth. However, the model’s r-squared is below .9, meaning it is likely not successful. Although since all the models are below it from the ranking results, this means that soil_depth is not a viable predictor to stream flow."
  }
]