---
title: "ESS 330 - Lab 6: Machine Learning in Hydrology"
author: "Ava Zelenz"
date: 04-17-2025
format: 
  html:
    self-contained: true
execute:
  echo: true
---

### Library Code -

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

### CAMEL Data Set Import -

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')
```

## I. Visualizing Relationships

### Question 1: Your Turn -

#### Data and PDF is downloaded and in my /data directory.

#### zero_q_frequency is when the frequency of days with Q = 0 mm/day.

### Exploratory Data Analysis -

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

### Question 2: Your Turn -

#### Map of Aridity

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "dodgerblue", high = "salmon") +
  labs(title = "Map of Gauge Aridity Values in the US") +
  ggthemes::theme_map()
```

#### Map of p_mean

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat, color = p_mean)) +
  borders("state", colour = "gray50") +
  geom_point() +
  scale_color_gradient(low = "palegreen", high = "#FF69B4") +
  labs(title = "Map of Gauge p_mean Values in the US") +
  ggthemes::theme_map()
```

#### Combined Map

```{r}
camels_long <- camels %>%
  pivot_longer(cols = c(aridity, p_mean), 
               names_to = "variable", 
               values_to = "value")

# Create the faceted plot
ggplot(data = camels_long, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = value)) +
  scale_color_gradient(low = "dodgerblue", high = "pink") +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Map of Gauge Values in the US") +
  ggthemes::theme_map()
```

## II. Model Preparation

#### Relationship between aridity, rainfall, and mean flow.

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

# Visual EDA

# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

# Test Log Transformation Graph
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

# Visualizing Log Transform with q_mean added
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

### A. Model Building

#### Splitting the Data

```{r}
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

#### Preprocessors: recipe

```{r}
# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

#### Naive base lm approach

```{r}
# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

#### Correct Version: prep -\> bake -\> predict

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

#### Model Evaluation: Statistical and visual

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)

ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

#### Using a workflow instead

```{r}
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients

# From the base implementation
summary(lm_base)$coefficients
```

#### Making Predictions

```{r}
#
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

## III. Model Evaluation

### Statistical & Visual

#### Define/Extract Default metrics

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

#### Scatter plot to visualize

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

### Switch it up

#### Random Forest model to predict streamflow

```{r}
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 
```

#### Predictions

```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

#### Model Evaluation: statistical and visual

```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)

ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

#### A workflowset approach

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

### Question 3: Your Turn!

#### Build a xgboost (engine) regression (mode) model using boost_tree & a neural network model using bag_mlp()

```{r}
# XGBoost model
xg_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression")

xg_workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(xg_model) |>
  fit(data = camels_train)

# Neural net model
nn_model <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode("regression")

nn_workflow <- workflow() |>
  add_recipe(rec) |>
  add_model(nn_model) |>
  fit(data = camels_train)
```

#### Evaluate these models

```{r}
# Making Predictions
xg_preds <- predict(xg_workflow, new_data = camels_test) |>
  bind_cols(camels_test)

nn_preds <- predict(nn_workflow, new_data = camels_test) |>
  bind_cols(camels_test)

# XGBoost metrics
metrics_xg <- xg_preds |>
  metrics(truth = logQmean, estimate = .pred)

# Neural Net metrics
metrics_nn <- nn_preds |>
  metrics(truth = logQmean, estimate = .pred)

#Visualizing 
bind_rows(
  xg_preds |> mutate(model = "XGBoost"),
  nn_preds |> mutate(model = "Neural Net")
) |>
  ggplot(aes(x = logQmean, y = .pred, color = model)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "Actual", y = "Predicted") +
  theme_minimal()
```

#### **Model Preference: Neural Net Model**

####The neural net model has the largest r-squared value (0.759). Of the 4 models, I would utilize the Neural Network Model because it outperforms the others when considering error and explanatory power while being stable.

### III. Build Your Own

#### Data Splitting

```{r}
set.seed(456)

camels_split <- initial_split(camels, prop = 0.75)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

camels |> 
  select(soil_depth_statsgo, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

```

#### Recipe used: (logQmean \~ soil_depth_statgso + p_mean) to see if the depth of the soil impacts the amount of streamflow.

```{r}
rec <-  recipe(logQmean ~ soil_depth_statsgo + p_mean, data = camels_train) %>%
  step_interact(terms = ~ soil_depth_statsgo:p_mean) |> 
  step_naomit(all_predictors(), all_outcomes())

baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ soil_depth_statsgo * p_mean, data = baked_data)
summary(lm_base)

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

#### Visualizing

```{r}
camels_long <- camels %>%
  pivot_longer(cols = c(soil_depth_statsgo, p_mean), 
               names_to = "variable", 
               values_to = "value")

# Create the faceted plot
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = soil_depth_statsgo)) +
  scale_color_gradient(low = "dodgerblue", high = "salmon") +
  labs(title = "Map of Gauge Soil Depth Values in the US") +
  ggthemes::theme_map()
```

#### Models -

```{r}
rf_model <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("regression")

nn_model <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode("regression")

xg_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression")

lm_model <- linear_reg() |>
  set_engine("lm") |>
  set_mode("regression")
```

#### Workflow Set

```{r}
complex_workflow <- workflow_set(
  preproc = list(simple = rec),
  models = list(
    linear_reg = lm_model,
    random_forest = rf_model,
    bagged_mlp = nn_model,
    xgboost = xg_model
  )
)

model_results <- complex_workflow |>
  workflow_map(resamples = camels_cv)
```

#### Evaluation

```{r}
autoplot(model_results)

summary(model_results)

rank_results(model_results, rank_metric = "rsq", select_best = TRUE)
```

#### Model Ranking Discussion -

#### With the ranking results, the random forest is the best model to find a correlation between the variables (soil_porosity & p_mean) and stream flow.

### Extract and Evaluate

#### Model Used: Random Forest

```{r}
rf_model <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("regression")

rf_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(rf_model) |>
  fit(data = camels_train) 

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)

rf_metrics <- rf_data |>
  metrics(truth = logQmean, estimate = .pred)

rf_metrics

# Plotting

ggplot(rf_data, aes(x = logQmean, y = .pred)) +
  geom_point(alpha = 0.6, color = "lightgreen") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "darkblue") +
  labs(
    title = "Random Forest: Actual vs Predicted Streamflow (logQmean) by Soil Depth and Precipitation",
    x = "Actual logQmean",
    y = "Predicted logQmean"
  ) +
  theme_minimal()
```

#### Result Discussion:

#### As the logQmean increases, the correlation becomes stronger for predictions of stream flow using predictors such as precipitation and soil depth. However, the model's r-squared is below .9, meaning it is likely not successful. Although since all the models are below it from the ranking results, this means that soil_depth is not a viable predictor to stream flow.
